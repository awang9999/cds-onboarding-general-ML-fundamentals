{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59e06ab-a54b-4110-b414-56bb0ba2ff18",
   "metadata": {},
   "source": [
    "# General Machine Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67fca2-b80c-4a9b-8e46-926dab37ac7a",
   "metadata": {},
   "source": [
    "Machine learning can be a complicated task. Luckily, many common functions have already been abstracted and made in to useful libraries by our predecessors! Let's make use of them by importing the required packages! If you get any errors, make sure to install the package in your (virtual) environment by either running `pip install [package_name]` or using another package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568249c-b363-4aef-a49b-8f0a1f8fd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de30d2-8fcd-4d5b-ba43-b14a982f49b3",
   "metadata": {},
   "source": [
    "This function generates a toy dataset. This is the 'target function' you will be trying to develop a machine learning model to emulate today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35e38e-62e3-4c68-8be1-9376392678dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    return xTr,yTr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bf868-8c66-45df-8912-066188df2ba5",
   "metadata": {},
   "source": [
    "### Step 1) Preliminary data analysis\n",
    "The following cell will help you visualize what the dataset looks like. What observations can we make about it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94381fe-248a-41ff-bf66-602c90bafb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "ndata = 300\n",
    "Xs,ys =spiraldata(ndata)\n",
    "plt.scatter(Xs[ys == 1, 0], Xs[ys == 1, 1], c='r')\n",
    "plt.scatter(Xs[ys != 1, 0], Xs[ys != 1, 1], c='b')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9f68a-e6ec-4588-ad29-9ffa15b709ff",
   "metadata": {},
   "source": [
    "### Step 1.5) Data Preprocessing\n",
    "\n",
    "The technique used here is a type of feature engineering that will make this dataset discernable to a linear classifier. I have provided this because it was what worked for the hypothesis class I have suggested later. Feel free to poke around, change the pre-processing method, or even delete it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8fcbc-f0fc-4619-96b4-2f92f8b94992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "new_feature_ratio = ndata;\n",
    "\n",
    "def relu(Y): return np.maximum(Y, 0)\n",
    "\n",
    "K = np.random.randn(Xs.shape[1], Xs.shape[1]*new_feature_ratio)\n",
    "b = np.random.randn(Xs.shape[1]*new_feature_ratio)\n",
    "\n",
    "NXs = relu(np.matmul(Xs,K) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54673def-6684-44a2-87d2-452817083661",
   "metadata": {},
   "source": [
    "This plot will provide some insight at what the pre-processing step did to the dataset. It's pretty hard to envision because the transformation done above is effectively a multi-dimensional extrusion of the data such that there exists some hyperplane separator to separate blue and red examples. Nevertheless, the projection of the higher-dimension extrusion transformation already shows a 'simpler' structure in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c39aa-f574-4f26-8532-026364e14bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(NXs[ys == 1, 0], NXs[ys == 1, 1], c='r')\n",
    "plt.scatter(NXs[ys != 1, 0], NXs[ys != 1, 1], c='b')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cd34-624a-4a6c-923c-0b79ea8714f8",
   "metadata": {},
   "source": [
    "### Step 2) Train-validate-test split\n",
    "In the next cell, shuffle and split your dataset into a training set and a testing set. Don't worry too much about a validation set for now. Although it is best practice to use the validation set for hyperparameter tuning later, this dataset is so primitive that such a practice is overkill. Suggested train-test splits are 70-80% training, 20-30% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ed063-e76c-4a8e-a377-e64a0ea55972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validate-test split\n",
    "\n",
    "# Shuffle your dataset using a permutation: np.random.permutation(parameters)\n",
    "# Use splice notation to create your training and testing datasets.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568b7e94-2c2b-483c-9706-def00f19504e",
   "metadata": {},
   "source": [
    "### Step 3) Hypothesis class selection\n",
    "\n",
    "As a hint for your success for this mini-project, I suggest that you consider the hypothesis class of kernelized support vector machines. The details of how this hypothesis class works is not important besides that it is a linear model, that is, it is really really good at separating linear data. But you might say... our dataset is clearly not linearly separable. \n",
    "\n",
    "### Step 4) Hyperparameter tuning\n",
    "That's where the kernel hyperparameter comes in. Try using the polynomial kernel, `poly`, or the radial basis function kernel, `rbf`, to make this linear classifier mathemagically work for nonlinear datasets. The other hyperparameters for a SVM is `C` which, in brief, controls how much overfitting you allow (higher C corresponds to allowing more overfitting). `gamma` is a hyperparameter that controls the distance a certain data point has influence over the decision boundary in the `rbf` kernel. `degree` controls the degree of the polynomial decision boundary used in the `poly` kernel. `max_iter` sets a maximum number of iterations. SVMs are trained iteratively, and we need to set a maximum so it doesn't just train forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ff860-497e-43da-aa7e-5c383a504d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested hypothesis class: Support Vector Machines\n",
    "\n",
    "# Example usage:\n",
    "# model = svm.SVC(parameters)\n",
    "# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# Suggested hyperparameter settings:\n",
    "# kernel = ['linear', 'poly', 'rbf']\n",
    "# C = [0.01, 0.1, 1.0, 10, 100, 100]\n",
    "# gamma = [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "# max_iter = [100, 500, 1000]\n",
    "\n",
    "# Training function: model.fit(examples, labels)\n",
    "\n",
    "model = None # CHANGE THIS LINE\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751445f-cafd-4e86-896f-91f5d3ec2200",
   "metadata": {},
   "source": [
    "### Step 5) Evaluate your model\n",
    "\n",
    "A common evaluation metric for a model is it's accuracy on the validation/testing set. Here is the mathematical formula for accuracy:\n",
    "\n",
    "$$accuracy = \\frac{\\text{# correct predictions}}{\\text{# incorrect predictions}}$$\n",
    "\n",
    "Try implementing this on your own and calculate and print the accuracy of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1ccc6-31a7-4300-afe0-1bdb382a826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b67bc-e2db-42ed-8cb9-2df8a212aeac",
   "metadata": {},
   "source": [
    "We also provide some code to visualize the decision boundary of your model. This should give a sense of how good it is at discerning between the blue and red examples in the spiral dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03956a2f-30a1-4d00-bdf9-e81b35b9d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the decision boundary of your model!\n",
    "x = Xs\n",
    "y = ys\n",
    "\n",
    "xmin, xmax = x[:,0].min()-0.1, x[:,0].max()+0.1\n",
    "ymin, ymax = x[:,1].min()-0.1, x[:,1].max()+0.1\n",
    "xx, yy = np.meshgrid(np.arange(xmin, xmax, 0.01), np.arange(ymin, ymax, 0.01))\n",
    "xnew = np.c_[xx.ravel(), yy.ravel()]\n",
    "ynew = model.predict(relu(np.matmul(xnew,K) + b)).reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, ynew, cmap='bwr', alpha=0.2)\n",
    "plt.scatter(Xs[ys == 1, 0], Xs[ys == 1, 1], c='r')\n",
    "plt.scatter(Xs[ys != 1, 0], Xs[ys != 1, 1], c='b')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850dc77-34e5-4b90-85cd-ab651b744cd5",
   "metadata": {},
   "source": [
    "### Step 6) Results\n",
    "\n",
    "How did your model perform? Were you able to achieve >99% accuracy? How does the decision boundary look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b716641-6a88-4d30-bc92-f1c1eaded10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8f08d-5971-49d9-b8a7-a320911a2848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
